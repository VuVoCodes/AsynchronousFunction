# Asynchronous Staleness Guided Multimodal Learning (ASGML) v2

**Updated Design Document for NeurIPS 2026 Submission**

Date: 2026-02-25

---

## Executive Summary

This document reflects the **evolved ASGML design** after extensive experimentation. The core insight shifted from "temporal separation via update skipping" to "probe-guided weak-modality boosting as a complement to existing gradient modulation methods."

**Key Result**: ASGML Boost + OGM-GE achieves **71.45% Â± 1.71%** on CREMA-D (3-frame), outperforming either method alone.

---

## 1. Research Gap (Refined)

### 1.1 Original Gap (Still Valid)
Existing methods address modality imbalance through **synchronous** interventions:
- **Gradient modulation** (OGM-GE, AGM, CGGM): Reweight/throttle gradients but update all modalities every step
- **Loss modification** (CGGM): Add direction alignment loss terms
- **Unimodal regularization** (MMPareto, ARL): Auxiliary losses, still synchronous

### 1.2 New Insight: Orthogonal Interventions

All existing methods focus on **throttling the dominant modality**:

```
OGM-GE:  dominant_grad *= tanh(...)  â†’ scale DOWN dominant
CGGM:    dominant gets lower B       â†’ scale DOWN dominant
AGM:     similar throttling mechanism
```

**Gap identified**: No method explicitly **boosts the weak modality**. These are orthogonal interventions:

| Intervention | Target | Direction |
|--------------|--------|-----------|
| Throttle dominant | Fast learner | Pull back |
| **Boost weak** | Slow learner | Push forward |

Combining both provides **two-sided pressure** toward balance.

### 1.3 Why Original ASGML (Frequency/Staleness) Didn't Work

The original design used dual-signal (gradient ratio + loss descent) to compute staleness Ï„:

```python
# Original: Normalized signal â†’ Ï„ values centered around mean
learning_speed[m] = Î² * grad_ratio + (1-Î²) * loss_ratio  # Both normalized
Ï„[m] = tau_base * learning_speed[m]  # Results in Ï„ â‰ˆ 1-2 for both
```

**Problem**: Normalization removes absolute difference information:
- Audio gradient = 100, Visual = 50 â†’ ratio = 1.33 vs 0.67
- After Ï„ computation and clamping â†’ Ï„_audio â‰ˆ 2, Ï„_visual â‰ˆ 2
- **No differentiation** â†’ binary skip rarely triggered

---

## 2. ASGML v2: Probe-Guided Continuous Boosting

### 2.1 Core Idea

Replace binary update skipping with **continuous gradient scaling** based on **decoupled probe monitoring**:

```
Original ASGML:  Skip updates for dominant (binary, rarely triggered)
ASGML v2:        Boost gradients for weak (continuous, always active)
```

### 2.2 Design Principles

1. **Decoupled Monitoring**: Probes observe encoder quality without affecting training
2. **Absolute Signal**: Use probe accuracy (not normalized ratios) to preserve gap information
3. **Continuous Scaling**: Smooth gradient modulation instead of binary skip
4. **Complementarity**: Design to work WITH existing methods (OGM-GE), not replace them

### 2.3 System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ASGML v2 ARCHITECTURE                          â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚   Audio     â”‚      â”‚   Visual    â”‚                              â”‚
â”‚  â”‚  Encoder    â”‚      â”‚  Encoder    â”‚                              â”‚
â”‚  â”‚ (ResNet18)  â”‚      â”‚ (ResNet18)  â”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚         â”‚                    â”‚                                      â”‚
â”‚         â–¼                    â–¼                                      â”‚
â”‚    features_a           features_v                                  â”‚
â”‚         â”‚                    â”‚                                      â”‚
â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                      â”‚
â”‚         â”‚        â”‚           â”‚                                      â”‚
â”‚         â”‚        â–¼           â”‚                                      â”‚
â”‚         â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚                                      â”‚
â”‚         â”‚   â”‚ Fusion  â”‚      â”‚                                      â”‚
â”‚         â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚                                      â”‚
â”‚         â”‚        â”‚           â”‚                                      â”‚
â”‚         â”‚        â–¼           â”‚                                      â”‚
â”‚         â”‚    logits          â”‚                                      â”‚
â”‚         â”‚        â”‚           â”‚                                      â”‚
â”‚         â”‚        â–¼           â”‚                                      â”‚
â”‚         â”‚   L_task (only)    â”‚     â—„â”€â”€ Loss unchanged              â”‚
â”‚         â”‚        â”‚           â”‚                                      â”‚
â”‚         â”‚        â–¼           â”‚                                      â”‚
â”‚         â”‚   backward()       â”‚                                      â”‚
â”‚         â”‚        â”‚           â”‚                                      â”‚
â”‚         â–¼        â–¼           â–¼                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚              GRADIENT MODIFICATION                       â”‚      â”‚
â”‚   â”‚                                                          â”‚      â”‚
â”‚   â”‚  1. OGM-GE (if enabled): Throttle dominant              â”‚      â”‚
â”‚   â”‚     audio_grad *= 0.6                                    â”‚      â”‚
â”‚   â”‚                                                          â”‚      â”‚
â”‚   â”‚  2. ASGML Boost: Boost weak                             â”‚      â”‚
â”‚   â”‚     visual_grad *= 1.5                                   â”‚      â”‚
â”‚   â”‚                                                          â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                    â”‚                                      â”‚
â”‚         â–¼                    â–¼                                      â”‚
â”‚   optimizer.step()                                                  â”‚
â”‚                                                                     â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â”‚
â”‚                                                                     â”‚
â”‚   PROBE MONITORING (Decoupled, Periodic)                           â”‚
â”‚                                                                     â”‚
â”‚   features.detach() â”€â”€â–º Linear Probes â”€â”€â–º accuracies               â”‚
â”‚                                              â”‚                      â”‚
â”‚                                              â–¼                      â”‚
â”‚                                    Compute boost scales             â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Method Details

### 3.1 Probe-Based Monitoring

**Architecture**: Lightweight linear probes (512 â†’ num_classes)

```python
class LinearProbe(nn.Module):
    def __init__(self, feature_dim, num_classes):
        self.classifier = nn.Linear(feature_dim, num_classes)  # ~3K params
```

**Safety Guarantee**: Complete decoupling via `.detach()`

```python
# CRITICAL: Probes NEVER backprop into encoders
feat = features[modality].detach().float()
logits = probe(feat)
loss.backward()  # Only updates probe weights
```

**Split-Batch Evaluation**: Honest signal estimation

```python
# Prevent probe overfitting on training batch
split = batch_size // 2
train_features = {m: f[:split] for m, f in features.items()}  # Train probes
eval_features = {m: f[split:] for m, f in features.items()}   # Evaluate probes
```

### 3.2 Continuous Boost Scaling

**Formula**:

```python
def get_continuous_scales(utilization_scores, alpha=0.5, scale_max=2.0):
    max_score = max(utilization_scores.values())
    min_score = min(utilization_scores.values())
    gap = max_score - min_score

    scales = {}
    for m in modalities:
        # Relative weakness: 0 for dominant, 1 for weakest
        rel_weakness = 1.0 - (scores[m] - min_score) / gap

        # Scale: 1.0 for dominant, up to scale_max for weakest
        scales[m] = 1.0 + alpha * rel_weakness
        scales[m] = min(scales[m], scale_max)

    return scales
```

**Example Calculation**:

```
Probe accuracies: audio = 0.65, visual = 0.35
Gap = 0.30, alpha = 0.5, scale_max = 2.0

Audio (dominant):
  rel_weakness = 1.0 - (0.65 - 0.35) / 0.30 = 0.0
  scale = 1.0 + 0.5 * 0.0 = 1.0  â† unchanged

Visual (weak):
  rel_weakness = 1.0 - (0.35 - 0.35) / 0.30 = 1.0
  scale = 1.0 + 0.5 * 1.0 = 1.5  â† boosted 50%
```

**EMA Smoothing** (stability):

```python
scale_new = ema_alpha * scale_computed + (1 - ema_alpha) * scale_old
# ema_alpha = 0.3 (default)
```

### 3.3 Comparison with CGGM

| Aspect | CGGM | ASGML v2 |
|--------|------|----------|
| Classifier/Probe coupling | Coupled (joint training) | Decoupled (`.detach()`) |
| Loss modification | Yes (`L = L_task + Î»*L_gm`) | No (`L = L_task` only) |
| Signal type | Improvement Î”Îµ (normalized) | Absolute accuracy |
| Intervention point | Before backward (in loss) | After backward (gradient scaling) |
| Direction modulation | Yes (via `L_gm`) | No (magnitude only) |
| Batch usage | Same batch | Split batch |
| Evaluation frequency | Every iteration | Periodic (`eval_freq`) |

**Key Insight**: CGGM's classifier is a **co-pilot** (steers training via loss). ASGML's probe is a **dashboard gauge** (displays info for external decision).

---

## 4. Theoretical Framing

### 4.1 Orthogonal Interventions

Let g_i denote the gradient for modality i. Existing methods apply:

```
OGM-GE:  g_dominant *= Î±  where Î± < 1  (throttle)
```

ASGML adds:

```
ASGML:   g_weak *= Î²  where Î² > 1  (boost)
```

Combined effect on gradient ratio:

```
Before:  ||g_dominant|| / ||g_weak|| = r  (imbalanced, r >> 1)
After:   ||Î± * g_dominant|| / ||Î² * g_weak|| = (Î±/Î²) * r

With Î±=0.6 (OGM-GE) and Î²=1.5 (ASGML):
  (0.6/1.5) * r = 0.4 * r  (2.5Ã— better balance)
```

### 4.2 Why Boosting Works

**Gradient magnitude affects learning speed**: Larger gradients â†’ faster parameter updates â†’ faster feature learning.

By boosting weak modality gradients:
1. Weak encoder updates more aggressively
2. Features become more discriminative faster
3. Fusion head learns to use both modalities

### 4.3 Why Decoupled Probes Matter

**Coupled classifiers (CGGM)** create feedback loops:
- Classifier loss affects encoder training
- Encoder changes affect classifier signal
- Potential for instability or mode collapse

**Decoupled probes (ASGML)** provide clean measurement:
- Probes observe but don't interfere
- Signal reflects true encoder quality
- No gradient contamination

---

## 5. Experimental Results

### 5.1 CREMA-D Results

| Method | Accuracy (1-frame) | Accuracy (3-frame) |
|--------|-------------------|-------------------|
| Baseline | 60.30% | 62.37% |
| OGM-GE | 62.47% | 69.14% |
| ASGML Boost | 62.90% | 65.52% |
| **ASGML Boost + OGM-GE** | **62.69%** | **71.45% Â± 1.71%** |

### 5.2 Key Findings

1. **Original ASGML (frequency/staleness) â‰ˆ baseline**: Binary skip rarely triggered due to normalization
2. **ASGML Boost alone**: Modest improvement, but less than OGM-GE
3. **Combination is best**: OGM-GE throttles dominant + ASGML boosts weak = orthogonal interventions

### 5.3 Why Combination Works

```
Gradient spectrum after interventions:

BEFORE:     Audio â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  Visual â–ˆâ–ˆâ–ˆâ–ˆ
            (dominant, large grad)      (weak, small grad)

OGM-GE:     Audio â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          Visual â–ˆâ–ˆâ–ˆâ–ˆ
            (throttled)                 (unchanged)

ASGML:      Audio â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  Visual â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
            (unchanged)                 (boosted)

COMBINED:   Audio â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              Visual â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
            (throttled)                 (boosted)

            NOW BALANCED!
```

---

## 6. Hyperparameters

| Parameter | Description | Default | Range |
|-----------|-------------|---------|-------|
| `alpha` | Boost strength | 0.5 | 0.1 - 1.0 |
| `scale_max` | Maximum boost scale | 2.0 | 1.5 - 3.0 |
| `eval_freq` | Probe evaluation frequency | 20 | 10 - 100 |
| `probe_train_steps` | Steps to train probe per eval | 10 | 5 - 50 |
| `ema_alpha` | EMA smoothing for scales | 0.3 | 0.1 - 0.5 |
| `probe_type` | Probe architecture | "linear" | linear, mlp_1layer |

---

## 7. Implementation Details

### 7.1 Training Loop Integration

```python
# Standard forward pass
logits, unimodal_logits, features = model(inputs, return_features=True)

# Standard loss (unchanged)
loss = criterion(logits, targets) + gamma * unimodal_loss

# Standard backward
loss.backward()

# OGM-GE gradient modulation (if enabled)
apply_ogm_ge(model, unimodal_logits, ...)

# Periodic probe evaluation
if step % eval_freq == 0:
    # Split batch for honest evaluation
    train_feats, eval_feats = split_batch(features)
    probe_manager.train_probes(train_feats, ...)
    probe_results = probe_manager.evaluate_probes(eval_feats, ...)

    # Compute boost scales from probe signal
    scales = scheduler.get_continuous_scales(probe_results)
    scheduler.update_continuous_scales(scales)  # EMA smooth

# Apply ASGML boost (every step, using cached scales)
for m in modalities:
    scale = scheduler.current_continuous_scales[m]
    for param in model.encoders[m].parameters():
        if param.grad is not None:
            param.grad.data.mul_(scale)

# Optimizer step
optimizer.step()
```

### 7.2 Model Architecture

```
Audio Input (B, 1, 128, T) â”€â”€â–º AudioEncoder (ResNet18) â”€â”€â–º (B, 512)
                                                              â”‚
Visual Input (B, 3, H, W) â”€â”€â–º VisualEncoder (ResNet18) â”€â”€â–º (B, 512)
                                                              â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                              ConcatFusion
                            (B, 1024) â†’ (B, 512)
                                    â”‚
                                    â–¼
                              Classifier
                              (B, 512) â†’ (B, 6)
                                    â”‚
                                    â–¼
                               logits

Probes (separate):
  features['audio'].detach() â”€â”€â–º LinearProbe â”€â”€â–º accuracy
  features['visual'].detach() â”€â”€â–º LinearProbe â”€â”€â–º accuracy
```

---

## 8. Ablation Studies (Required)

| Experiment | What It Tests |
|------------|---------------|
| Baseline | Lower bound |
| OGM-GE only | SOTA synchronous throttling |
| ASGML Boost only | Boosting without throttling |
| **ASGML + OGM-GE** | **Full contribution** |
| ASGML + CGGM | Complementarity with other methods |
| Different alpha values | Boost strength sensitivity |
| Different scale_max | Maximum boost limit |
| Different eval_freq | Probe evaluation frequency |
| Linear vs MLP probe | Probe architecture |
| With/without EMA | Scale smoothing importance |
| With/without split-batch | Honest signal importance |

---

## 9. Paper Positioning

### 9.1 Title Options

1. "Probe-Guided Gradient Boosting for Balanced Multimodal Learning"
2. "ASGML: Boosting Weak Modalities via Decoupled Utilization Monitoring"
3. "Complementary Gradient Modulation: Throttle + Boost for Multimodal Balance"

### 9.2 Key Claims

1. **Decoupled probes** provide clean utilization signal without training interference
2. **Boosting weak modalities** is orthogonal to throttling dominant modalities
3. **Combined approach** (OGM-GE + ASGML) achieves SOTA on CREMA-D
4. **Simple linear probes** are sufficient for effective monitoring

### 9.3 Contribution Summary

| Contribution | Description |
|--------------|-------------|
| **Methodological** | First method to explicitly boost weak modality gradients |
| **Architectural** | Decoupled probe design with split-batch evaluation |
| **Empirical** | Demonstrates complementarity of throttle + boost |
| **Practical** | Minimal overhead (~6K probe params, periodic evaluation) |

---

## 10. Open Questions

1. **Theoretical analysis**: Can we prove convergence for combined OGM-GE + ASGML?
2. **Scaling to N modalities**: Does boost-weak generalize to 3+ modalities?
3. **Probe alternatives**: Can we use gradient-based signals instead of probes?
4. **Dynamic alpha**: Should boost strength adapt during training?
5. **Other combinations**: Does ASGML complement CGGM, AGM, etc.?

---

## 11. Timeline

| Phase | Task | Status |
|-------|------|--------|
| Phase 1 | Baseline + OGM-GE reproduction | âœ… Complete |
| Phase 2 | Original ASGML (frequency/staleness) | âœ… Complete (didn't work) |
| Phase 3 | ASGML Boost (continuous mode) | âœ… Complete |
| Phase 4 | Multi-seed validation | âœ… Complete (71.45% Â± 1.71%) |
| Phase 5 | Additional datasets (KS, AVE) | ğŸ”„ Pending |
| Phase 6 | 3-modality test (CMU-MOSEI) | ğŸ”„ Pending |
| Phase 7 | Ablation studies | ğŸ”„ Pending |
| Phase 8 | Paper writing | ğŸ”„ Pending |

---

## 12. Code Pointers

| Component | File | Key Function/Class |
|-----------|------|-------------------|
| Probes | `src/models/probes.py` | `ProbeManager`, `LinearProbe` |
| Scheduler | `src/losses/asgml.py` | `ASGMLScheduler.get_continuous_scales()` |
| Training | `scripts/train.py` | Main training loop (lines 749-990) |
| OGM-GE | `scripts/train.py` | `apply_ogm_ge_modulation()` |
| Config | `configs/cremad.yaml` | ASGML hyperparameters |

---

## Appendix A: Evolution from v1 to v2

### What Changed

| Aspect | ASGML v1 | ASGML v2 |
|--------|----------|----------|
| Core mechanism | Binary update skipping | Continuous gradient scaling |
| Signal | Dual-signal (normalized) | Probe accuracy (absolute) |
| Trigger | Rarely (Ï„ â‰ˆ 1-2 for all) | Always active |
| Direction | Throttle dominant | Boost weak |
| Standalone performance | â‰ˆ Baseline | Modest improvement |
| With OGM-GE | Not tested | **71.45%** (best) |

### Why v1 Failed

1. **Normalization issue**: Dual-signal normalized by mean â†’ lost absolute gap info
2. **Binary decision**: Skip/update is coarse; continuous scaling is smoother
3. **Wrong direction**: Throttling alone is already done by OGM-GE

### Why v2 Works

1. **Absolute signal**: Probe accuracy preserves the utilization gap
2. **Continuous scaling**: Smooth, always-active intervention
3. **Complementary**: Boosts weak instead of throttling dominant
4. **Decoupled**: Clean measurement without training interference
