# CREMA-D Dataset Configuration
# Audio-visual emotion recognition (6 classes)
# Audio is typically the dominant modality

# Dataset
dataset:
  name: cremad
  root: /path/to/CREMA-D  # Update this path
  num_classes: 6
  modalities: [audio, visual]

# Model
model:
  backbone: resnet18
  pretrained: true
  feature_dim: 512
  fusion_type: concat
  fusion_dim: 512

# Training
training:
  epochs: 100
  batch_size: 64
  num_workers: 4

  # Optimizer
  optimizer: sgd
  lr: 0.001
  momentum: 0.9
  weight_decay: 0.0001

  # LR scheduler
  scheduler: step
  step_size: 40
  gamma: 0.1

# ASGML hyperparameters
asgml:
  enabled: true

  # Mode settings
  asgml_mode: frequency  # "frequency" or "staleness" (for adaptive mode)

  # Staleness/frequency bounds
  tau_base: 2.0
  tau_min: 1.0
  tau_max: 8.0  # Higher for CREMA-D as per ARL paper (T=8)
  max_staleness_ratio: 3.0  # κ: max τ_i/τ_j ratio (Stale Fusion constraint)

  # Signal weighting
  beta: 0.5  # Weight for gradient magnitude (1-β for loss descent)

  # Gradient compensation
  lambda_comp: 0.1

  # Unimodal regularization weight (matching ARL γ=4)
  gamma: 4.0

  # Adaptive mode threshold
  threshold_delta: 0.1  # Utilization gap threshold to trigger adaptation

  # Probe settings
  probe_type: linear  # "linear" or "mlp_1layer"
  probe_lr: 0.001
  eval_freq: 100  # Evaluate probes every N batches
  probe_train_steps: 50  # Steps to train probe per evaluation

# Logging
logging:
  log_interval: 20
  save_interval: 10
  tensorboard: true

# Evaluation
evaluation:
  metrics: [accuracy, f1_macro]
