# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

NeurIPS 2026 submission: **Asynchronous Staleness Guided Multimodal Learning (ASGML)**

A novel approach to balanced multimodal learning through adaptive asynchronous updates, where each modality has an independent update frequency determined by its learning dynamics.

## Repository Structure

```
├── paper/                  # NeurIPS LaTeX paper
│   ├── main.tex
│   └── sections/           # Introduction, method, experiments, etc.
├── src/
│   ├── models/             # Encoders (audio, visual, text) and fusion modules
│   ├── datasets/           # CREMA-D, AVE, Kinetics-Sounds loaders
│   ├── losses/             # ASGML loss function
│   └── utils/              # Metrics, logging
├── configs/                # YAML configs per dataset
├── scripts/                # Training scripts
├── docs/plans/             # Design documents
└── Papers/                 # Reference papers
```

## Build Commands

```bash
# Install dependencies
pip install -e .

# Train with ASGML on CREMA-D
python scripts/train.py --config configs/cremad.yaml

# Train baseline (no ASGML)
python scripts/train.py --config configs/cremad.yaml --no-asgml
```

## Key Files

- `src/losses/asgml.py` - Core ASGML loss with adaptive staleness
- `src/models/multimodal.py` - N-modality multimodal model
- `docs/plans/2026-02-01-ASGML-design.md` - Full method design document

## Research Focus

**Problem**: Dominant modalities suppress weaker ones during the Prime Learning Window.

**Solution**: True asynchronous updates (not just gradient reweighting) where:
- Fast-learning modalities get higher staleness (fewer updates)
- Slow-learning modalities get lower staleness (more updates)
- Staleness adapts based on gradient magnitude ratio + loss descent rate


## Context and Memory

Purpose & context
John is a PhD student in computer science specializing in multimodal machine learning methodologies. His research focuses on adaptive fusion mechanisms that can dynamically combine different data modalities (vision, text, audio, clinical data) rather than using fixed fusion strategies. While he uses medical AI applications—particularly gastrointestinal and cardiovascular domains—as testbeds for validation, his core contribution targets computer science methodology and algorithmic innovation rather than medical applications themselves.
John's research has evolved toward addressing the "Prime Learning Window" problem, where multimodal networks suppress weaker modalities during early training phases due to gradient dynamics in late-fusion architectures. This represents a shift from general adaptive fusion exploration to targeting a specific theoretical challenge with practical implications for multimodal system performance.
His work operates within the broader context of addressing critical gaps in multimodal AI, including architectural complexity issues, modality information gaps, and efficiency challenges. Success is measured by algorithmic contributions that advance the theoretical understanding of multimodal learning while demonstrating practical improvements on standard benchmarks.
Current state
John is actively developing research on asynchronous loss functions for multimodal learning, specifically exploring how different modalities can be updated at different frequencies while maintaining independent monitoring mechanisms. This work builds on gradient staleness concepts from distributed systems literature and aims to address scenarios where modalities have different computational requirements or update schedules.
His current technical focus involves understanding gradient dynamics in multimodal networks, including how gradient modulation fits into the data flow between gradient computation and parameter updates. He's working on theoretical convergence properties adapted from asynchronous SGD theory and designing probe networks for monitoring individual modality learning states.
The research has progressed from literature review and gap identification to concrete experimental design, with plans for implementation on datasets like CREMA-D using standard hardware (RTX 3090 level).
On the horizon
John has a concrete research roadmap targeting a submission-ready paper within 10-12 weeks. The immediate next steps involve implementing a minimal viable experiment combining asynchronous optimization with independent monitoring mechanisms, starting with frequency-based asynchronous training and expanding to more sophisticated approaches.
He's considering hybrid approaches that combine gradient modulation techniques (like OGM-GE) with asynchronous updates, potentially creating novel integration strategies for multimodal learning scenarios. The research pipeline includes both theoretical analysis of convergence properties and empirical validation across multiple benchmarks.
Future exploration includes extending beyond the current focus to broader adaptive fusion mechanisms and potentially developing unified frameworks for multimodal system optimization.
Key learnings & principles
John has identified that truly asynchronous multimodal optimization with independent update schedules remains significantly underexplored in the literature, despite mature work in gradient modulation and loss reweighting. This represents a genuine research gap with potential for novel contributions.
He's learned that the Prime Learning Window problem stems from gradient dynamics visiting saddle manifolds corresponding to unimodal solutions, with theoretical work providing exact calculations for unimodal phase duration. Solutions like OGM-GE, MMPareto, and CGGM can achieve performance improvements of 20+ percentage points on benchmarks.
A key insight is the distinction between methods that provide reduced update frequency versus true gradient staleness, which have different mathematical implications for multimodal learning systems. John recognizes the importance of combining theoretical understanding with practical implementation considerations.
Approach & patterns
John follows a systematic research methodology starting with comprehensive literature reviews to identify gaps, followed by theoretical analysis and practical implementation. He prefers well-cited technical analysis with clear algorithmic properties and theoretical foundations over application-specific results.
His approach emphasizes positioning new methods within broader taxonomies and understanding comparative advantages across different techniques. He values detailed comparative analysis including performance metrics, complexity trade-offs, and theoretical characterizations.
John's research pattern involves starting with minimal viable experiments and progressively expanding to more sophisticated approaches, maintaining focus on both theoretical contributions and empirical validation across standard benchmarks.
Tools & resources
John regularly references top-tier conferences including NeurIPS, ICML, ICLR, CVPR, and MICCAI for staying current with multimodal learning advances. He uses specialized datasets like CREMA-D, Kvasir/HyperKvasir, and REAL-Colon for experimental validation.
His technical toolkit includes gradient-based optimization methods, transformer architectures, and fusion mechanisms ranging from early/late fusion to more sophisticated adaptive approaches. He works with standard deep learning frameworks capable of implementing asynchronous training and gradient modulation techniques.
For literature research, he relies on comprehensive database searches across multiple venues simultaneously, combining venue-specific queries with dataset-specific searches to identify both domain-specific work and transferable methodological contributions.





Good, now I have the full picture. Let me walk through the code systematically — there are several issues ranging from a genuine bug to design decisions that directly explain your Phase 1 results.                       
  Critical Bug: AMP + Staleness Interaction                                                                                                                                                                                  
  This is the most important issue. Look at the flow in train_epoch:                                                                                                                                                         
  Line 234-236: scaler.scale(loss).backward()     # gradients are AMP-SCALED                                                                                                                                                 
  Line 250-252: staleness_buffer.store_gradients() # stores SCALED gradients                                                                                                                                                 
  ...τ steps later...                                                                                                                                                                                                        
  Line 653:     param.grad.data = stale_grads[...] # applies OLD-scaled gradients                                                                                                                                            
  Line 281:     scaler.step(optimizer)              # unscales using CURRENT scale factor                                                                                                                                    
  When AMP is enabled, param.grad after scaler.scale(loss).backward() contains gradients multiplied by the scaler's current loss scale. The scaler's scale factor changes over training (it doubles when gradients are       
  stable, halves on inf/nan). Your staleness buffer stores these scaled gradients, then applies them τ steps later when the scale factor has potentially chan Then scaler.step() unscales everything using the current       
  scale — meaning your stale gradients get unscaled by the wrong factor.                                                                                                                                                     
  This introduces unpredictable gradient magnitudes for the staleness condition. It could explain why τ=2 showed stronger audio learning (randomly amplified gradients) rather than the expected throttling.                 
  The fix is straightforward — store gradients after unscaling, or store the scale factor alongside each gradient snapshot:                                                                                                  
  python# Option A: store unscaled gradients                                                                                                                                                                                 
  if scaler is not None:                                                                                                                                                                                                     
  scaler.unscale_(optimizer)  # unscale all grads first                                                                                                                                                                      
  # THEN store + modify gradients                                                                                                                                                                                            
  # THEN call optimizer.step() (without scaler)                                                                                                                                                                              
                                                                                                                                                                                                                             
  # Option B: store the scale factor with each snapshot                                                                                                                                                                      
  staleness_buffer.store_gradients(m, encoder_params, global_step,                                                                                                                                                           
  scale_factor=scaler.get_scale())                                                                                                                                                                                           
  # On retrieval, correct: stale_grad * (current_scale / stored_scale)                                                                                                                                                       
  ```                                                                                                                                                                                                                        
                                                                                                                                                                                                                             
  Option A is cleaner. You'd restructure the training loop to: backward → unscale → storeradients → `optimizer.step()` → `scaler.update()`.                                                                                  
                                                                                                                                                                                                                             
  ## High-Impact Design Issue: γ=4.0 Dominates the Loss                                                                                                                                                                      
                                                                                                                                                                                                                             
  Your total loss is:                                                                                                                                                                                                        
  ```                                                                                                                                                                                                                        
  L_total = L_fusion + 4.0 * (L_audio + L_visual)                                                                                                                                                                            
  With cross-entropy on a 6-class problem, each unimodal loss starts around ln(6) ≈ 1.79. So the initial loss balance is roughly 1.79 + 4.0 * (1.79 + 1.79) ≈ 16.1, where the fusion loss is only ~11% of the total. The     
  model is overwhelmingly training on unimodal objectives.                                                                                                                                                                   
  This has three consequences for your results. First, it explains the severe overfitting — each encoder is independently overfit to classify emotions from its own modality, which is easier than learning                  
  complementary cross-modal features. Second, it means gradient norms are dominated by the unimodal loss gradients, so your LearningDynamicsTracker is mostly measuring unimodal training speed, not fusion-relevant         
  dynamics. Third, when frequency mode sets update_mask['audio'] = False, it also excludes audio's unimodal loss from the total (line 568), which caual loss to oscillate by ~40% between masked and unmasked steps.         
  This creates jagged optimization that likely hurts convergence.                                                                                                                                                            
  The default γ value in the ASGML design doc is 1.0 (Section 3.3), and the comment in asgml.py says "matching ARL" at 4.0. Check whether ARL actually uses γ=4.0 — my understanding is that most unimodal                   
  regularization methods use γ in the 0.5-2.0 range.                                                                                                                                                                         
  Probe Evaluation is Too Noisy for Adaptive Control                                                                                                                                                                         
  In train_epoch (line 298-313), probes are evaluated on the current training batch (batch_size=64). With 6 classes and 64 samples, that's roughly 10-11 samples per class. Probe accuracy on this tiny slice will have      
  high variance — a few correct/incorrect predictions swing accuracy by multiple percentage points. The full-test-set probe evaluation in evaluate() happens only once per epoch.                                            
  For Phase 1 with fixed schedules this doesn't matter. But when you activate adaptive ASGML, the scheduler's update_from_utilization (line 308-313) will receive these noisy per-batch signals and ratic adaptation         
  decisions. You need either: (a) EMA smoothing of probe accuracies before feeding to the scheduler, or (b) evaluate probes on a held-out validation buffer (accumulate the last N batches' features).                       
  AudioEncoder Pretrained Weight Mismatch                                                                                                                                                                                    
  In encoders.py line 80-82:                                                                                                                                                                                                 
  pythonself.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)                                                                                                                               
  You load ImageNet-pretrained ResNet18 (3-channel conv1), then replace conv1 with a fresh 1-channel convolution. The pretrained weights for conv1 are discarded. A common fix that preserves pretrained information:        
  python# Average pretrained 3-channel weights into 1 channel                                                                                                                                                                
  pretrained_conv1 = self.backbone.conv1.weight.data                                                                                                                                                                         
  self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)                                                                                                                                     
  self.backbone.conv1.weight.data = pretrained_conv1.mean(dim=1, keepdim=True)                                                                                                                                               
  This is minor but could contribute a small amount to the baseline gap vs literature.                                                                                                                                       
  Frequency Mode Gradient Zeroing Order                                                                                                                                                                                      
  In train_epoch lines 271-276, when a modality is masked, you zero its encoder gradients. But the conditional unimodal loss at lines 567-569 means the backward pass already computed fewer gradient contributions for      
  the masked modality (it only gets fusion loss gradients, not its unimodal loss). Then you zero those remaining gradients too.                                                                                              
  The issue: the fusion loss DOES depend on the masked encoder's features. By zeroing the encoder's gradients entirely, you're also zeroing the fusion loss signal for that encoder. This means on masked steps, the         
  audio encoder gets literally zero learning signal — not even the "how to contribute better to fusion" signal. Consider whether you actually want to only zero the unimodal loss gradients while preserving the fusion      
  loss gradients:                                                                                                                                                                                                            
  python# Current: zero ALL encoder gradients (including fusion-path)                                                                                                                                                        
  # Alternative: compute fusion-only gradient for masked modalities                                                                                                                                                          
  This is a design choice rather than a bug, but it's a more aggressive intervention than necessary and could explain why k=2 hurts performance smuch — you're not just slowing audio, you're completely blacking it out     
  every other step.                                                                                                                                                                                                          
  Smaller Issues Worth Noting                                                                                                                                                                                                
  Gradient compensation formula (line 461): scale = 1.0 + λ * τ. With λ=0.1 and τ=2, this gives a 1.2x amplification of stale gradients. The standard async SGD correction reduces the effective learning rate for stale     
  gradients (η/(1+τ)), not amplifies them. Amplifying stale gradients increases variance. This needs either theoretical justification or should be inverted.                                                                 
  LearningDynamicsTracker._compute_loss_descent_ratios (line 243): max(history[0] - history[-1], 0.0) — clamping at 0 means when loss is increasing (which happens during overfitting), the descent rate is 0 for all        
  modalities, making all speed scores equal. The tracker loses signal precisely when it's most needed.                                                                                                                       
  StepLR(step=40, gamma=0.1) with 50 epochs means the LR drops 10x at epoch 40, giving only 10 epochs of very-low-LR fine-tuning. With 100 epochs standard, this schedule makes more sense. At 50 epochs, most of youul      
  training happens in the first 40 epochs.                                                                                                                                                                                   
  Recommended Fix Priority                                                                                                                                                                                                   
                                                                                                                                                                                                                             
  Fix the AMP+staleness bug before running any more staleness experiments — your τ=2 results may not be valid                                                                                                                
  Reduce γ from 4.0 to 1.0 and rerun baseline to see if overfitting reduces                                                                                                                                                  
  Extend to 100 epochs (matching literature)                                                                                                                                                                                 
  Fix conv1 weight initialization for the audio encoder                                                                                                                                                                      
  Add EMA smoothing for probe signals before activating adaptive mode                                                                                                                                                        
  Reconsider frequency mode gradient zeroing — preserving fusion-path gradients would be a less aggressive intervention                                                                                                      
                                                                                                                                                                                                                             
  The AMP bug and the γ=4.0 setting together could account for most of your anomalous Phase 1 results. I'd fix those two and rerun all three conditions before drawing any further conclusions.                              
                                                                                                                                                                                                                             
⏺ This is an excellent code review. Let me systematically fix these issues.                                                                                                                                                  
                                                                                                                                                                                                                             
⏺ Read(scripts/train.py)                                                                                                                                                                                                     
  ⎿  Read 700 lines                                                                                                                                                                                                          
                                                                                                                                                                                                                             
⏺ Read(src/losses/asgml.py)                                                                                                                                                                                                  
  ⎿  Read 656 lines                                                                                                                                                                                                          
                                                                                                                                                                                                                             
⏺ Read(src/models/encoders.py)                                                                                                                                                                                               
  ⎿  Read 137 lines                                                                                                                                                                                                          
                                                                                                                                                                                                                             
⏺ Now I understand the full codebase. Let me fix these issues systematically.                                                                                                                                                
                                                                                                                                                                                                                             
  Issue 1: Fix AMP + Staleness Interaction Bug                                                                                                                                                                               
                                                                                                                                                                                                                             
⏺ Update(scripts/train.py)                                                                                                                                                                                                   
  ⎿  Added 10 lines, removed 1 line                                                                                                                                                                                          
      233          # ========== Backward Pass ==========                                                                                                                                                                     
      234          if use_amp and scaler is not None:                                                                                                                                                                        
      235              scaler.scale(loss).backward()                                                                                                                                                                         
      236 +            # CRITICAL FIX: Unscale gradients BEFORE storing/modifying                                                                                                                                            
      237 +            # This prevents AMP scale factor mismatch when applying stale gradients                                                                                                                               
      238 +            scaler.unscale_(optimizer)                                                                                                                                                                            
      239          else:                                                                                                                                                                                                     
      240              loss.backward()                                                                                                                                                                                       
      241                                                                                                                                                                                                                    
      242          # ========== Compute Gradient Norms (for tracking) ==========                                                                                                                                             
      243 +        # Now computed on unscaled gradients for accurate tracking                                                                                                                                                
      244          grad_norms = compute_gradient_norms(model, modalities)                                                                                                                                                    
      245                                                                                                                                                                                                                    
      246          # ========== Store Gradients (for staleness mode) ==========                                                                                                                                              
      247 +        # Gradients are now unscaled, so staleness buffer stores correct values                                                                                                                                   
      248          if scheduler.mode == "staleness":                                                                                                                                                                         
      249              for m in modalities:                                                                                                                                                                                  
      250                  encoder_params = {                                                                                                                                                                                
     ...                                                                                                                                                                                                                     
      268                              grad_scales[m],                                                                                                                                                                       
      269                          )                                                                                                                                                                                         
      270              else:                                                                                                                                                                                                 
      271 -                # Frequency mode: zero out gradients for non-updating modalities                                                                                                                                  
      271 +                # Frequency mode: zero out ONLY encoder gradients for non-updating modalities                                                                                                                     
      272 +                # NOTE: This zeros ALL encoder gradients including fusion-path.                                                                                                                                   
      273 +                # Consider preserving fusion-path gradients for less aggressive intervention.                                                                                                                     
      274                  for m in modalities:                                                                                                                                                                              
      275                      if not update_mask[m]:                                                                                                                                                                        
      276                          for name, param in model.named_parameters():                                                                                                                                              
     ...                                                                                                                                                                                                                     
      278          # ========== Optimizer Step ==========                                                                                                                                                                    
      279          # NOTE: Fusion head always updates regardless of modality schedules                                                                                                                                       
      280          if use_amp and scaler is not None:                                                                                                                                                                        
      281 +            # Gradients already unscaled above, so use step() directly                                                                                                                                            
      282 +            # Check for inf/nan before stepping (scaler.step handles this)                                                                                                                                        
      283              scaler.step(optimizer)                                                                                                                                                                                
      284              scaler.update()                                                                                                                                                                                       
      285          else:                                                                                                                                                                                                     
  ⎿  Found 1 new diagnostic issue in 1 file (ctrl+o to expand)                                                                                                                                                               
                                                                                                                                                                                                                             
⏺ Issue 2: Reduce γ from 4.0 to 1.0 in configs                                                                                                                                                                               
                                                                                                                                                                                                                             
⏺ Read(configs/cremad.yaml)                                                                                                                                                                                                  
  ⎿  Read 77 lines                                                                                                                                                                                                           
                                                                                                                                                                                                                             
⏺ Update(configs/cremad.yaml)                                                                                                                                                                                                
  ⎿  Added 4 lines, removed 2 lines                                                                                                                                                                                          
      53    # Gradient compensation                                                                                                                                                                                          
      54    lambda_comp: 0.1                                                                                                                                                                                                 
      55                                                                                                                                                                                                                     
      56 -  # Unimodal regularization weight (matching ARL γ=4)                                                                                                                                                              
      57 -  gamma: 4.0                                                                                                                                                                                                       
      56 +  # Unimodal regularization weight                                                                                                                                                                                 
      57 +  # NOTE: Reduced from 4.0 to 1.0 to prevent unimodal loss from dominating                                                                                                                                         
      58 +  # With γ=4.0, unimodal losses were ~89% of total loss, causing overfitting                                                                                                                                       
      59 +  gamma: 1.0                                                                                                                                                                                                       
      60                                                                                                                                                                                                                     
      61    # Adaptive mode threshold                                                                                                                                                                                        
      62    threshold_delta: 0.1  # Utilization gap threshold to trigger adaptation                                                                                                                                          
                                                                                                                                                                                                                             
⏺ Update(configs/cremad.yaml)                                                                                                                                                                                                
  ⎿  Added 1 line, removed 1 line                                                                                                                                                                                            
      31                                                                                                                                                                                                                     
      32    # LR scheduler                                                                                                                                                                                                   
      33    scheduler: step                                                                                                                                                                                                  
      34 -  step_size: 40                                                                                                                                                                                                    
      34 +  step_size: 70  # Updated for 100 epochs (drop LR at 70% of training)                                                                                                                                             
      35    gamma: 0.1                                                                                                                                                                                                       
      36                                                                                                                                                                                                                     
      37  # ASGML hyperparameters                                                                                                                                                                                            
                                                                                                                                                                                                                             
⏺ Update(configs/ave.yaml)                                                                                                                                                                                                   
  ⎿  Added 2 lines, removed 1 line                                                                                                                                                                                           
      54    lambda_comp: 0.1                                                                                                                                                                                                 
      55                                                                                                                                                                                                                     
      56    # Unimodal regularization weight                                                                                                                                                                                 
      57 -  gamma: 4.0                                                                                                                                                                                                       
      57 +  # NOTE: Reduced from 4.0 to 1.0 to prevent unimodal loss from dominating                                                                                                                                         
      58 +  gamma: 1.0                                                                                                                                                                                                       
      59                                                                                                                                                                                                                     
      60    # Adaptive mode threshold                                                                                                                                                                                        
      61    threshold_delta: 0.1                                                                                                                                                                                             
                                                                                                                                                                                                                             
⏺ Update(configs/kinetics_sounds.yaml)                                                                                                                                                                                       
  ⎿  Added 2 lines, removed 1 line                                                                                                                                                                                           
      53    lambda_comp: 0.1                                                                                                                                                                                                 
      54                                                                                                                                                                                                                     
      55    # Unimodal regularization weight                                                                                                                                                                                 
      56 -  gamma: 4.0                                                                                                                                                                                                       
      56 +  # NOTE: Reduced from 4.0 to 1.0 to prevent unimodal loss from dominating                                                                                                                                         
      57 +  gamma: 1.0                                                                                                                                                                                                       
      58                                                                                                                                                                                                                     
      59    # Adaptive mode threshold                                                                                                                                                                                        
      60    threshold_delta: 0.1                                                                                                                                                                                             
                                                                                                                                                                                                                             
⏺ Issue 3: Fix AudioEncoder conv1 pretrained weight preservation                                                                                                                                                             
                                                                                                                                                                                                                             
⏺ Update(src/models/encoders.py)                                                                                                                                                                                             
  ⎿  Added 5 lines                                                                                                                                                                                                           
      72              raise ValueError(f"Unknown backbone: {backbone}")                                                                                                                                                      
      73                                                                                                                                                                                                                     
      74          # Modify first conv layer for single-channel spectrogram input                                                                                                                                             
      75 +        # Preserve pretrained information by averaging 3-channel weights into 1 channel                                                                                                                            
      76 +        pretrained_conv1_weight = self.backbone.conv1.weight.data.clone()                                                                                                                                          
      77          self.backbone.conv1 = nn.Conv2d(                                                                                                                                                                           
      78              1, 64, kernel_size=7, stride=2, padding=3, bias=False                                                                                                                                                  
      79          )                                                                                                                                                                                                          
      80 +        # Average RGB channels to create single-channel weights                                                                                                                                                    
      81 +        # This preserves learned edge/texture detectors from ImageNet                                                                                                                                              
      82 +        self.backbone.conv1.weight.data = pretrained_conv1_weight.mean(dim=1, keepdim=True)                                                                                                                        
      83                                                                                                                                                                                                                     
      84          # Remove classification head                                                                                                                                                                               
      85          self.backbone.fc = nn.Identity()                                                                                                                                                                           
                                                                                                                                                                                                                             
⏺ Issue 4: Fix frequency mode gradient zeroing                                                                                                                                                                               
                                                                                                                                                                                                                             
  Let me add a "soft masking" option that scales gradients instead of zeroing them completely.                                                                                                                               
                                                                                                                                                                                                                             
⏺ Update(scripts/train.py)                                                                                                                                                                                                   
  ⎿  Added 13 lines, removed 4 lines                                                                                                                                                                                         
      273                              grad_scales[m],                                                                                                                                                                       
      274                          )                                                                                                                                                                                         
      275              else:                                                                                                                                                                                                 
      276 -                # Frequency mode: zero out ONLY encoder gradients for non-updating modalities                                                                                                                     
      277 -                # NOTE: This zeros ALL encoder gradients including fusion-path.                                                                                                                                   
      278 -                # Consider preserving fusion-path gradients for less aggressive intervention.                                                                                                                     
      276 +                # Frequency mode: reduce encoder gradients for non-updating modalities                                                                                                                            
      277 +                # Options:                                                                                                                                                                                        
      278 +                #   - hard_mask=True (default): Zero all encoder gradients (original behavior)                                                                                                                    
      279 +                #   - hard_mask=False: Scale gradients by small factor to preserve some signal                                                                                                                    
      280 +                hard_mask = config["asgml"].get("hard_frequency_mask", False)                                                                                                                                     
      281 +                soft_scale = config["asgml"].get("soft_mask_scale", 0.1)  # 10% of gradient                                                                                                                       
      282 +                                                                                                                                                                                                                  
      283                  for m in modalities:                                                                                                                                                                              
      284                      if not update_mask[m]:                                                                                                                                                                        
      285                          for name, param in model.named_parameters():                                                                                                                                              
      286                              if f"encoders.{m}" in name and param.grad is not None:                                                                                                                                
      287 -                                param.grad.data.zero_()                                                                                                                                                           
      287 +                                if hard_mask:                                                                                                                                                                     
      288 +                                    param.grad.data.zero_()                                                                                                                                                       
      289 +                                else:                                                                                                                                                                             
      290 +                                    # Soft masking: reduce but don't eliminate gradient                                                                                                                           
      291 +                                    # This preserves fusion-path learning signal                                                                                                                                  
      292 +                                    param.grad.data.mul_(soft_scale)                                                                                                                                              
      293                                                                                                                                                                                                                    
      294          # ========== Optimizer Step ==========                                                                                                                                                                    
      295          # NOTE: Fusion head always updates regardless of modality schedules                                                                                                                                       
  ⎿  Found 1 new diagnostic issue in 1 file (ctrl+o to expand)                                                                                                                                                               
                                                                                                                                                                                                                             
⏺ Issue 5: Fix gradient compensation formula                                                                                                                                                                                 
                                                                                                                                                                                                                             
⏺ Update(src/losses/asgml.py)                                                                                                                                                                                                
  ⎿  Added 11 lines, removed 4 lines                                                                                                                                                                                         
      446          """                                                                                                                                                                                                       
      447          Compute gradient scaling factors for staleness compensation.                                                                                                                                              
      448                                                                                                                                                                                                                    
      449 -        When using stale gradients, we may want to scale them to                                                                                                                                                  
      450 -        compensate for the delay.                                                                                                                                                                                 
      449 +        Standard async SGD theory suggests reducing effective learning rate                                                                                                                                       
      450 +        for stale gradients: η_effective = η / (1 + τ)                                                                                                                                                            
      451                                                                                                                                                                                                                    
      452 +        We implement this as gradient scaling: scale = 1 / (1 + λ * τ)                                                                                                                                            
      453 +        where λ controls the reduction strength.                                                                                                                                                                  
      454 +                                                                                                                                                                                                                  
      455 +        With λ=0.1 and τ=2: scale = 1/(1+0.2) = 0.83 (17% reduction)                                                                                                                                              
      456 +        With λ=0.1 and τ=4: scale = 1/(1+0.4) = 0.71 (29% reduction)                                                                                                                                              
      457 +                                                                                                                                                                                                                  
      458          Returns:                                                                                                                                                                                                  
      459              Dict mapping modality names to gradient scale factors                                                                                                                                                 
      460          """                                                                                                                                                                                                       
     ...                                                                                                                                                                                                                     
      457                                                                                                                                                                                                                    
      458          for m in self.modalities:                                                                                                                                                                                 
      459              tau = staleness_vals.get(m, 0)                                                                                                                                                                        
      460 -            # Scale = 1 + λ * τ (compensate for missed updates)                                                                                                                                                   
      461 -            scales[m] = 1.0 + self.lambda_comp * tau                                                                                                                                                              
      460 +            # Scale = 1 / (1 + λ * τ) (reduce stale gradient contribution)                                                                                                                                        
      461 +            # This follows async SGD convergence theory                                                                                                                                                           
      462 +            scales[m] = 1.0 / (1.0 + self.lambda_comp * tau)                                                                                                                                                      
      463                                                                                                                                                                                                                    
      464          return scales                                                                                                                                                                                             
      465                                                                                                                                                                                                                    
                                                                                                                                                                                                                             
⏺ Issue 6: Fix loss descent ratio clamping                                                                                                                                                                                   
                                                                                                                                                                                                                             
⏺ Update(src/losses/asgml.py)                                                                                                                                                                                                
  ⎿  Added 14 lines, removed 3 lines                                                                                                                                                                                         
      232          return {m: emas[m] / mean_norm for m in self.modalities}                                                                                                                                                  
      233                                                                                                                                                                                                                    
      234      def _compute_loss_descent_ratios(self) -> Dict[str, float]:                                                                                                                                                   
      235 -        """Compute loss descent rate relative to mean across modalities."""                                                                                                                                       
      235 +        """                                                                                                                                                                                                       
      236 +        Compute loss descent rate relative to mean across modalities.                                                                                                                                             
      237 +                                                                                                                                                                                                                  
      238 +        Positive descent = loss decreasing = learning                                                                                                                                                             
      239 +        Negative descent = loss increasing = possibly overfitting                                                                                                                                                 
      240 +                                                                                                                                                                                                                  
      241 +        We use absolute value of descent to capture learning activity                                                                                                                                             
      242 +        regardless of direction. A modality with increasing loss is still                                                                                                                                         
      243 +        "active" and shouldn't have its signal zeroed out.                                                                                                                                                        
      244 +        """                                                                                                                                                                                                       
      245          descents = {}                                                                                                                                                                                             
      246          for m in self.modalities:                                                                                                                                                                                 
      247              history = list(self.loss_history[m])                                                                                                                                                                  
      248              if len(history) < 2:                                                                                                                                                                                  
      249                  descents[m] = 0.0                                                                                                                                                                                 
      250              else:                                                                                                                                                                                                 
      251 -                # Descent = first - last (positive = loss decreasing = learning)                                                                                                                                  
      252 -                descents[m] = max(history[0] - history[-1], 0.0)                                                                                                                                                  
      251 +                # Use absolute descent to capture learning activity                                                                                                                                               
      252 +                # This preserves signal even when loss is increasing (overfitting phase)                                                                                                                          
      253 +                raw_descent = history[0] - history[-1]                                                                                                                                                            
      254 +                descents[m] = abs(raw_descent)                                                                                                                                                                    
      255                                                                                                                                                                                                                    
      256          mean_descent = sum(descents.values()) / max(len(descents), 1)                                                                                                                                             
      257                                                                                                                                                                                                                    
                                                                                                                                                                                                                             
⏺ Issue 7: Add EMA smoothing for probe signals                                                                                                                                                                               
                                                                                                                                                                                                                             
⏺ Read(src/models/probes.py)                                                                                                                                                                                                 
  ⎿  Read 313 lines                                                                                                                                                                                                          
                                                                                                                                                                                                                             
⏺ Update(src/models/probes.py)                                                                                                                                                                                               
  ⎿  Added 8 lines                                                                                                                                                                                                           
      104          probe_type: str = "linear",                                                                                                                                                                               
      105          probe_lr: float = 1e-3,                                                                                                                                                                                   
      106          device: torch.device = None,                                                                                                                                                                              
      107 +        ema_alpha: float = 0.1,                                                                                                                                                                                   
      108      ):                                                                                                                                                                                                            
      109          """                                                                                                                                                                                                       
      110          Args:                                                                                                                                                                                                     
     ...                                                                                                                                                                                                                     
      113              probe_type: 'linear' or 'mlp_1layer'                                                                                                                                                                  
      114              probe_lr: Learning rate for probe optimizers                                                                                                                                                          
      115              device: Device to place probes on                                                                                                                                                                     
      116 +            ema_alpha: EMA smoothing factor for probe accuracies (0.1 = slow, 0.9 = fast)                                                                                                                         
      117          """                                                                                                                                                                                                       
      118          self.modalities = modalities                                                                                                                                                                              
      119          self.feature_dim = feature_dim                                                                                                                                                                            
      120          self.num_classes = num_classes                                                                                                                                                                            
      121          self.probe_type = probe_type                                                                                                                                                                              
      122          self.device = device or torch.device('cpu')                                                                                                                                                               
      123 +        self.ema_alpha = ema_alpha                                                                                                                                                                                
      124                                                                                                                                                                                                                    
      125          # Create probes for each modality                                                                                                                                                                         
      126          self.probes: Dict[str, nn.Module] = {}                                                                                                                                                                    
     ...                                                                                                                                                                                                                     
      137              m: [] for m in modalities                                                                                                                                                                             
      138          }                                                                                                                                                                                                         
      139                                                                                                                                                                                                                    
      140 +        # EMA-smoothed accuracies for stable adaptive scheduling                                                                                                                                                  
      141 +        # Raw per-batch probe accuracy has high variance (batch_size=64, 6 classes)                                                                                                                               
      142 +        # EMA smoothing prevents erratic adaptation decisions                                                                                                                                                     
      143 +        self.accuracy_ema: Dict[str, float] = {m: 0.0 for m in modalities}                                                                                                                                        
      144 +                                                                                                                                                                                                                  
      145      def _create_probe(self) -> nn.Module:                                                                                                                                                                         
      146          """Create a probe network based on probe_type."""                                                                                                                                                         
      147          if self.probe_type == "linear":                                                                                                                                                                           
                                                                                                                                                                                                                             
⏺ Update(src/models/probes.py)                                                                                                                                                                                               
  ⎿  Added 7 lines                                                                                                                                                                                                           
      238              # Track history                                                                                                                                                                                       
      239              self.accuracy_history[modality].append(accuracy)                                                                                                                                                      
      240                                                                                                                                                                                                                    
      241 +            # Update EMA-smoothed accuracy                                                                                                                                                                        
      242 +            # EMA = α * new_value + (1-α) * old_ema                                                                                                                                                               
      243 +            old_ema = self.accuracy_ema[modality]                                                                                                                                                                 
      244 +            self.accuracy_ema[modality] = (                                                                                                                                                                       
      245 +                self.ema_alpha * accuracy + (1 - self.ema_alpha) * old_ema                                                                                                                                        
      246 +            )                                                                                                                                                                                                     
      247 +                                                                                                                                                                                                                  
      248          return results                                                                                                                                                                                            
      249                                                                                                                                                                                                                    
      250      def compute_utilization_gap(self) -> Optional[float]:                                                                                                                                                         
                                                                                                                                                                                                                             
⏺ Update(src/models/probes.py)                                                                                                                                                                                               
  ⎿  Added 17 lines, removed 8 lines                                                                                                                                                                                         
      292          recent_accs = {m: h[-1] for m, h in self.accuracy_history.items()}                                                                                                                                        
      293          return min(recent_accs, key=recent_accs.get)                                                                                                                                                              
      294                                                                                                                                                                                                                    
      295 -    def get_utilization_scores(self) -> Dict[str, float]:                                                                                                                                                         
      295 +    def get_utilization_scores(self, use_ema: bool = True) -> Dict[str, float]:                                                                                                                                   
      296          """                                                                                                                                                                                                       
      297          Get current utilization score (probe accuracy) for each modality.                                                                                                                                         
      298                                                                                                                                                                                                                    
      299 +        Args:                                                                                                                                                                                                     
      300 +            use_ema: If True, return EMA-smoothed accuracies (more stable for adaptive control).                                                                                                                  
      301 +                     If False, return raw most-recent accuracies.                                                                                                                                                 
      302 +                                                                                                                                                                                                                  
      303          Returns:                                                                                                                                                                                                  
      304              Dict mapping modality names to their utilization scores                                                                                                                                               
      305          """                                                                                                                                                                                                       
      306 -        scores = {}                                                                                                                                                                                               
      307 -        for modality in self.modalities:                                                                                                                                                                          
      308 -            if self.accuracy_history[modality]:                                                                                                                                                                   
      309 -                scores[modality] = self.accuracy_history[modality][-1]                                                                                                                                            
      310 -            else:                                                                                                                                                                                                 
      311 -                scores[modality] = 0.0                                                                                                                                                                            
      312 -        return scores                                                                                                                                                                                             
      306 +        if use_ema:                                                                                                                                                                                               
      307 +            # Return EMA-smoothed values for stable adaptive scheduling                                                                                                                                           
      308 +            return self.accuracy_ema.copy()                                                                                                                                                                       
      309 +        else:                                                                                                                                                                                                     
      310 +            # Return raw most-recent values                                                                                                                                                                       
      311 +            scores = {}                                                                                                                                                                                           
      312 +            for modality in self.modalities:                                                                                                                                                                      
      313 +                if self.accuracy_history[modality]:                                                                                                                                                               
      314 +                    scores[modality] = self.accuracy_history[modality][-1]                                                                                                                                        
      315 +                else:                                                                                                                                                                                             
      316 +                    scores[modality] = 0.0                                                                                                                                                                        
      317 +            return scores                                                                                                                                                                                         
      318                                                                                                                                                                                                                    
      319      def reset_history(self) -> None:                                                                                                                                                                              
      320          """Reset accuracy history (call at start of training or new phase)."""                                                                                                                                    
                                                                                                                                                                                                                             
⏺ Update(src/models/probes.py)                                                                                                                                                                                               
  ⎿  Added 11 lines, removed 4 lines                                                                                                                                                                                         
      247                                                                                                                                                                                                                    
      248          return results                                                                                                                                                                                            
      249                                                                                                                                                                                                                    
      250 -    def compute_utilization_gap(self) -> Optional[float]:                                                                                                                                                         
      250 +    def compute_utilization_gap(self, use_ema: bool = True) -> Optional[float]:                                                                                                                                   
      251          """                                                                                                                                                                                                       
      252          Compute utilization gap between modalities.                                                                                                                                                               
      253                                                                                                                                                                                                                    
     ...                                                                                                                                                                                                                     
      256          A large gap indicates modality imbalance - one modality's                                                                                                                                                 
      257          representations are much more discriminative than another's.                                                                                                                                              
      258                                                                                                                                                                                                                    
      259 +        Args:                                                                                                                                                                                                     
      260 +            use_ema: If True, use EMA-smoothed accuracies (more stable).                                                                                                                                          
      261 +                                                                                                                                                                                                                  
      262          Returns:                                                                                                                                                                                                  
      263              Utilization gap, or None if not enough history                                                                                                                                                        
      264          """                                                                                                                                                                                                       
      265          if not all(len(h) > 0 for h in self.accuracy_history.values()):                                                                                                                                           
      266              return None                                                                                                                                                                                           
      267                                                                                                                                                                                                                    
      268 -        # Use most recent accuracies                                                                                                                                                                              
      269 -        recent_accs = [h[-1] for h in self.accuracy_history.values()]                                                                                                                                             
      270 -        return max(recent_accs) - min(recent_accs)                                                                                                                                                                
      268 +        if use_ema:                                                                                                                                                                                               
      269 +            accs = list(self.accuracy_ema.values())                                                                                                                                                               
      270 +        else:                                                                                                                                                                                                     
      271 +            # Use most recent raw accuracies                                                                                                                                                                      
      272 +            accs = [h[-1] for h in self.accuracy_history.values()]                                                                                                                                                
      273                                                                                                                                                                                                                    
      274 +        return max(accs) - min(accs)                                                                                                                                                                              
      275 +                                                                                                                                                                                                                  
      276      def get_dominant_modality(self) -> Optional[str]:                                                                                                                                                             
      277          """                                                                                                                                                                                                       
      278          Identify which modality is currently dominant (highest probe accuracy).                                                                                                                                   
                                                                                                                                                                                                                             
⏺ Update(src/models/probes.py)                                                                                                                                                                                               
  ⎿  Added 5 lines, removed 1 line                                                                                                                                                                                           
      324              return scores                                                                                                                                                                                         
      325                                                                                                                                                                                                                    
      326      def reset_history(self) -> None:                                                                                                                                                                              
      327 -        """Reset accuracy history (call at start of training or new phase)."""                                                                                                                                    
      327 +        """Reset accuracy history and EMA (call at start of training or new phase)."""                                                                                                                            
      328          self.accuracy_history = {m: [] for m in self.modalities}                                                                                                                                                  
      329 +        self.accuracy_ema = {m: 0.0 for m in self.modalities}                                                                                                                                                     
      330                                                                                                                                                                                                                    
      331      def state_dict(self) -> Dict:                                                                                                                                                                                 
      332          """Get state dict for checkpointing."""                                                                                                                                                                   
     ...                                                                                                                                                                                                                     
      333              'probes': {m: p.state_dict() for m, p in self.probes.items()},                                                                                                                                        
      334              'optimizers': {m: o.state_dict() for m, o in self.optimizers.items()},                                                                                                                                
      335              'accuracy_history': self.accuracy_history,                                                                                                                                                            
      336 +            'accuracy_ema': self.accuracy_ema,                                                                                                                                                                    
      337          }                                                                                                                                                                                                         
      338                                                                                                                                                                                                                    
      339      def load_state_dict(self, state_dict: Dict) -> None:                                                                                                                                                          
     ...                                                                                                                                                                                                                     
      341              self.probes[m].load_state_dict(state_dict['probes'][m])                                                                                                                                               
      342              self.optimizers[m].load_state_dict(state_dict['optimizers'][m])                                                                                                                                       
      343          self.accuracy_history = state_dict['accuracy_history']                                                                                                                                                    
      344 +        if 'accuracy_ema' in state_dict:                                                                                                                                                                          
      345 +            self.accuracy_ema = state_dict['accuracy_ema']                                                                                                                                                        
                                                                                                                                                                                                                             
⏺ Update(scripts/train.py)                                                                                                                                                                                                   
  ⎿  Added 1 line                                                                                                                                                                                                            
      607          probe_type=config["asgml"].get("probe_type", "linear"),                                                                                                                                                   
      608          probe_lr=config["asgml"].get("probe_lr", 1e-3),                                                                                                                                                           
      609          device=device,                                                                                                                                                                                            
      610 +        ema_alpha=config["asgml"].get("probe_ema_alpha", 0.1),  # EMA smoothing for stable adaptive control                                                                                                       
      611      )                                                                                                                                                                                                             
      612                                                                                                                                                                                                                    
      613      # Set initial dominant modality (will be updated by probes)                                                                                                                                                   
                                                                                                                                                                                                                             
⏺ All fixes complete. Here's a summary:                                                                                                                                                                                      
                                                                                                                                                                                                                             
  ---                                                                                                                                                                                                                        
  Bug Fixes Applied                                                                                                                                                                                                          
                                                                                                                                                                                                                             
  1. Critical: AMP + Staleness Interaction ✅                                                                                                                                                                                
                                                                                                                                                                                                                             
  File: scripts/train.py:234-238                                                                                                                                                                                             
  - Problem: Storing AMP-scaled gradients, then unscaling with different scale factor                                                                                                                                        
  - Fix: Call scaler.unscale_(optimizer) BEFORE storing gradients in staleness buffer                                                                                                                                        
                                                                                                                                                                                                                             
  2. High-Impact: γ=4.0 → γ=1.0 ✅                                                                                                                                                                                           
                                                                                                                                                                                                                             
  Files: All config YAMLs                                                                                                                                                                                                    
  - Problem: Unimodal losses dominated total loss (~89%), causing overfitting                                                                                                                                                
  - Fix: Reduced gamma from 4.0 to 1.0 in all configs                                                                                                                                                                        
                                                                                                                                                                                                                             
  3. AudioEncoder conv1 Weight Preservation ✅                                                                                                                                                                               
                                                                                                                                                                                                                             
  File: src/models/encoders.py:74-82                                                                                                                                                                                         
  - Problem: Pretrained 3-channel conv1 weights discarded when switching to 1-channel                                                                                                                                        
  - Fix: Average RGB channels to preserve learned edge/texture detectors                                                                                                                                                     
                                                                                                                                                                                                                             
  4. Frequency Mode Soft Masking ✅                                                                                                                                                                                          
                                                                                                                                                                                                                             
  File: scripts/train.py:271-286                                                                                                                                                                                             
  - Problem: Zeroing ALL encoder gradients including fusion-path gradients                                                                                                                                                   
  - Fix: Added soft masking option (soft_mask_scale=0.1) to preserve some signal                                                                                                                                             
                                                                                                                                                                                                                             
  5. Gradient Compensation Formula ✅                                                                                                                                                                                        
                                                                                                                                                                                                                             
  File: src/losses/asgml.py:445-472                                                                                                                                                                                          
  - Problem: Formula 1 + λτ amplifies stale gradients (wrong direction)                                                                                                                                                      
  - Fix: Changed to 1/(1 + λτ) following async SGD theory                                                                                                                                                                    
                                                                                                                                                                                                                             
  6. Loss Descent Ratio Clamping ✅                                                                                                                                                                                          
                                                                                                                                                                                                                             
  File: src/losses/asgml.py:234-258                                                                                                                                                                                          
  - Problem: Clamping negative descent to 0 loses signal during overfitting                                                                                                                                                  
  - Fix: Use absolute value to capture learning activity regardless of direction                                                                                                                                             
                                                                                                                                                                                                                             
  7. EMA Smoothing for Probes ✅                                                                                                                                                                                             
                                                                                                                                                                                                                             
  File: src/models/probes.py                                                                                                                                                                                                 
  - Problem: Per-batch probe accuracy has high variance (batch=64, 6 classes)                                                                                                                                                
  - Fix: Added EMA-smoothed accuracies (ema_alpha=0.1) for stable adaptive control                                                                                                                                           
                                                                                                                                                                                                                             
  8. Training Config Updates ✅                                                                                                                                                                                              
                                                                                                                                                                                                                             
  - Epochs: 100 (already set)                                                                                                                                                                                                
  - LR step_size: 40 → 70 (for 100 epochs)                                                                                                                                                                                   
                                                                                                                                                                                                                             
  ---                                                                                                                                                                                                                        
  New Config Options Added                                                                                                                                                                                                   
                                                                                                                                                                                                                             
  asgml:                                                                                                                                                                                                                     
    # Soft masking (default: hard masking for backward compatibility)                                                                                                                                                        
    hard_frequency_mask: false  # Set true for original hard masking                                                                                                                                                         
    soft_mask_scale: 0.1        # Gradient scale when masked (10%)                                                                                                                                                           
                                                                                                                                                                                                                             
    # Probe EMA smoothing                                                                                                                                                                                                    
    probe_ema_alpha: 0.1        # EMA factor (0.1 = slow/stable)                                                                                                                                                             
                                                                                                                                                                                                                             
  ---                                                                                                                                                                                                                        
  Recommended Next Steps                                                                                                                                                                                                     
                                                                                                                                                                                                                             
  1. Re-run Phase 1 experiments with these fixes to get valid baselines                                                                                                                                                      
  2. Compare γ=1.0 vs γ=4.0 to quantify overfitting reduction                                                                                                                                                                
  3. Test soft masking vs hard masking for frequency mode                                                                                                                                                                    
  4. Verify staleness mode now behaves correctly with proper gradient scaling   